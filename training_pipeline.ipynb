{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##### Authors: Alexander Mo & Tommaso Lucarelli\n",
    "\n",
    "# Training pipeline for LSTM stock market price prediction model\n",
    "This pipeline reads data from the feature store and adjusts it according to a 'feature view' to fit the data to an interpretable format w.r.t. the LSTM model input. Upon completing the training phase, the model is uploaded to Hugging Face where it can be used for inference in the web application.\n",
    "\n",
    "Code is written to run on Google Colab."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "\n",
    "%cd /gdrive/MyDrive/Scalable/Project/feature_store"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9pLufa097AS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Feature view processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXYyoW2x9jw7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_sequences(data, sequence_length, prediction_length):\n",
    "  X, y = [], []\n",
    "  c = data.iloc[0, 7]\n",
    "  i = 0\n",
    "  while i < (len(data) - sequence_length - prediction_length):\n",
    "    if ((i + sequence_length + prediction_length) < (len(data) - sequence_length - prediction_length)) and (data.iloc[(i + sequence_length + prediction_length), 7] != c):\n",
    "      i = i + sequence_length + prediction_length\n",
    "      c = data.iloc[i, 7]\n",
    "      print(i, c)\n",
    "    X.append(data.iloc[i:i+sequence_length, [0,1,2,3,4,5,6,8,9,10,11]])\n",
    "    y.append(data.iloc[i+sequence_length:i+sequence_length+prediction_length, 3]) \n",
    "    i += 1\n",
    "\n",
    "  return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_p4TqI1dAp7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"small_test.csv\")\n",
    "\n",
    "# Select the columns we want to scale\n",
    "scaled_data = data[['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits', 'Diff', 'EPS Estimate', 'Reported EPS', 'Offset']]\n",
    "\n",
    "max_close = data.max(axis=0)[3]\n",
    "min_close = data.min(axis=0)[3]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(scaled_data)\n",
    "\n",
    "# put the scaled data back into the dataframe\n",
    "data[['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits', 'Diff', 'EPS Estimate', 'Reported EPS', 'Offset']] = scaled_data\n",
    "\n",
    "sequence_length = 240  # Number of timesteps in each input sequence -> 10 days\n",
    "prediction_length = 24  # Number of timesteps to predict ->  1 day\n",
    "\n",
    "X, y = generate_sequences(data, sequence_length, prediction_length)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0B6D760cApXK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "output_shape = y_train.shape[1:]\n",
    "batch_size = 32\n",
    "epochs = 200\n",
    "\n",
    "print(input_shape)\n",
    "print(output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b3fQ6ap94cy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjQLjY7fAUgx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "\n",
    "def build_CONV_LSTM_model(input_shape, output_shape):\n",
    "    # Build the neural network layer by layer\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
    "\n",
    "    convlstm = tfkl.Bidirectional(tfkl.LSTM(128, return_sequences=True))(input_layer)\n",
    "    convlstm = tfkl.Conv1D(256, 3, padding='same', activation='relu')(convlstm)\n",
    "    convlstm = tfkl.MaxPool1D()(convlstm)\n",
    "    convlstm = tfkl.Bidirectional(tfkl.LSTM(256, return_sequences=True))(convlstm)\n",
    "    convlstm = tfkl.Conv1D(512, 3, padding='same', activation='relu')(convlstm)\n",
    "    convlstm = tfkl.GlobalAveragePooling1D()(convlstm)\n",
    "    convlstm = tfkl.Dropout(.5)(convlstm)\n",
    "\n",
    "    output_layer = tfkl.Dense(output_shape[-1], activation='relu')(convlstm)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.MeanSquaredError(), optimizer=tfk.optimizers.Adam(learning_rate=1e-4), metrics=['mae'])\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmZpLq9lARzM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = build_CONV_LSTM_model(input_shape, output_shape)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_split=.1,\n",
    "    callbacks = [\n",
    "        #Early stopping to avoid overfitting\n",
    "        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True),\n",
    "        #Reduce learning rate to do fine tuning in the last epochs\n",
    "        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-5)\n",
    "    ]\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovupEi0Qq-fc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#save the model in format h5 on google drive\n",
    "model.save(\"model_definitive.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#save the model to huggingface"
   ],
   "metadata": {
    "id": "h5Ss3XYZkovZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#save the model to huggingface\n",
    "! pip install huggingface_hub"
   ],
   "metadata": {
    "id": "PS-__n4qRQWO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "drive.flush_and_unmount()"
   ],
   "metadata": {
    "id": "CrZ_wqCphGbl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login, push_to_hub_keras\n",
    "\n",
    "notebook_login()"
   ],
   "metadata": {
    "id": "jfJQXbQnRb3v",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "push_to_hub_keras(model, \"stock_market_model\", token = \"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\")"
   ],
   "metadata": {
    "id": "UTM8u5LYRsdB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNfBzo26ri-k",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RU6Rvo01ridb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = tfk.models.load_model(\"model_small.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vF9H-gTKeqi_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Make predictions on the test data using your model\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "#inverse transform\n",
    "predictions = predictions*(max_close-min_close)+min_close\n",
    "actuals = y_test*(max_close-min_close)+min_close\n",
    "X_test_2 = X_test[:, :, 3]*(max_close-min_close)+min_close\n",
    "\n",
    "#metrics\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(\"mse\", mse)\n",
    "print(\"mae\", mae)\n",
    "\n",
    "#concatenate\n",
    "predictions = np.concatenate((X_test_2, predictions), axis=1)\n",
    "actuals = np.concatenate((X_test_2, actuals), axis=1)\n",
    "\n",
    "for i in range (len(predictions)):\n",
    "  # Create a scatter plot of the actual values\n",
    "  plt.plot(actuals[i,:], c='b', label='Actual')\n",
    "\n",
    "  # Create a line plot of the predicted values\n",
    "  plt.plot(predictions[i,:], c='r', label='Predicted')\n",
    "\n",
    "  # Set the x-axis range\n",
    "  plt.xlim([200, 264])\n",
    "\n",
    "  # Add a legend to the plot\n",
    "  plt.legend(loc='lower right')\n",
    "\n",
    "  # Show the plot\n",
    "  plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}